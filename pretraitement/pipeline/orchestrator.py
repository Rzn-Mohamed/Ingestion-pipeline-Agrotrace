"""
Orchestrator - Planification et ex√©cution du pipeline ETL
"""

import os
import logging
import psycopg2
from dotenv import load_dotenv
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.interval import IntervalTrigger
from datetime import datetime

from pipeline.bronze import BronzeExtractor
from pipeline.silver import SilverTransformer
from pipeline.gold import GoldLoader

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Chargement des variables d'environnement
load_dotenv()


class ETLOrchestrator:
    """Orchestrateur du pipeline ETL de nettoyage de donn√©es"""
    
    def __init__(self):
        self.db_connection = None
        self.bronze_extractor = None
        self.silver_transformer = None
        self.gold_loader = None
        self.scheduler = BlockingScheduler()
    
    def connect_database(self):
        """√âtablit la connexion √† la base de donn√©es"""
        try:
            self.db_connection = psycopg2.connect(
                host=os.getenv("DB_HOST", "localhost"),
                port=os.getenv("DB_PORT", "5432"),
                database=os.getenv("DB_NAME", "agrotrace_db"),
                user=os.getenv("DB_USER", "admin"),
                password=os.getenv("DB_PASSWORD", "password")
            )
            logger.info("Connexion √† la base de donn√©es √©tablie")
            
            # Initialiser les composants du pipeline
            self.bronze_extractor = BronzeExtractor(self.db_connection)
            self.silver_transformer = SilverTransformer()
            self.gold_loader = GoldLoader(self.db_connection)
            
            # Cr√©er la table de donn√©es nettoy√©es
            self.gold_loader.create_clean_table()
            
        except Exception as e:
            logger.error(f"Erreur de connexion √† la base de donn√©es: {e}")
            raise
    
    def run_etl_pipeline(self):
        """Ex√©cute le pipeline ETL complet"""
        try:
            start_time = datetime.now()
            logger.info("=" * 60)
            logger.info(f"D√©marrage du pipeline ETL - {start_time}")
            logger.info("=" * 60)
            
            # BRONZE: Extraction des donn√©es brutes
            raw_df = self.bronze_extractor.extract_raw_data(batch_size=1000)
            
            if raw_df is None or raw_df.empty:
                logger.info("Aucune donn√©e √† traiter, fin du cycle")
                return
            
            # Sauvegarder les IDs pour la mise √† jour ult√©rieure
            processed_ids = raw_df['id'].tolist()
            
            # SILVER: Nettoyage et transformation
            cleaned_df = self.silver_transformer.transform(raw_df)
            
            # GOLD: Chargement des donn√©es nettoy√©es
            loaded_count = self.gold_loader.load_clean_data(cleaned_df)
            
            # Marquer les donn√©es comme nettoy√©es
            updated_count = self.gold_loader.mark_as_cleaned(processed_ids)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info("=" * 60)
            logger.info(f"Pipeline ETL termin√© en {duration:.2f}s")
            logger.info(f"Enregistrements trait√©s: {loaded_count}")
            logger.info(f"Enregistrements marqu√©s: {updated_count}")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"Erreur lors de l'ex√©cution du pipeline: {e}", exc_info=True)
    
    def start_scheduler(self):
        """D√©marre le planificateur pour ex√©cuter le pipeline toutes les 5 minutes"""
        logger.info("D√©marrage du planificateur ETL")
        logger.info("Fr√©quence: Toutes les 5 minutes")
        
        # Ajouter le job au planificateur
        self.scheduler.add_job(
            func=self.run_etl_pipeline,
            trigger=IntervalTrigger(minutes=5),
            id='etl_cleaning_job',
            name='Nettoyage des donn√©es de capteurs',
            replace_existing=True
        )
        
        # Ex√©cuter imm√©diatement au d√©marrage
        logger.info("Ex√©cution initiale du pipeline...")
        self.run_etl_pipeline()
        
        # D√©marrer le planificateur
        logger.info("Planificateur actif - En attente du prochain cycle")
        try:
            self.scheduler.start()
        except (KeyboardInterrupt, SystemExit):
            logger.info("Arr√™t du planificateur demand√©")
            self.cleanup()
    
    def cleanup(self):
        """Nettoie les ressources"""
        logger.info("Nettoyage des ressources...")
        
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("Planificateur arr√™t√©")
        
        if self.db_connection:
            self.db_connection.close()
            logger.info("Connexion √† la base de donn√©es ferm√©e")


def main():
    """Point d'entr√©e principal"""
    logger.info("üöÄ D√©marrage du Worker ETL - Nettoyage des Donn√©es de Capteurs")
    logger.info("=" * 60)
    
    orchestrator = ETLOrchestrator()
    
    try:
        orchestrator.connect_database()
        orchestrator.start_scheduler()
    except KeyboardInterrupt:
        logger.info("‚ö†Ô∏è Interruption par l'utilisateur")
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}", exc_info=True)
    finally:
        orchestrator.cleanup()
        logger.info("üëã Worker ETL arr√™t√©")


if __name__ == "__main__":
    main()
